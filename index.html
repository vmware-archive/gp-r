<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Pivotal | R by pivotalsoftware</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Pivotal | R</h1>
      <h2 class="project-tagline">A place for all things Pivotal &amp; R</h2>
      <a href="https://github.com/pivotalsoftware/gp-r" class="btn">View on GitHub</a>
      <a href="https://github.com/pivotalsoftware/gp-r/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/pivotalsoftware/gp-r/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="topics-covered" class="anchor" href="#topics-covered" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Topics covered</h1>

<ul>
<li><a href="#overview">Overview</a></li>
<li>
<a href="#plr">PL/R on Greenplum &amp; HAWQ</a>

<ul>
<li>
<a href="#plr_gettingstarted">Getting Started</a>

<ul>
<li><a href="#plr_arch">PL/R Architecture</a></li>
<li><a href="#installation">PL/R Installation</a></li>
<li><a href="#permissions">Note on Permissions</a></li>
</ul>
</li>
<li>
<a href="#packages">Leveraging R Packages</a>

<ul>
<li><a href="#plr_packages_check">Checking R Package Availability</a></li>
<li><a href="#plr_packages_install">Installing R Packages</a></li>
<li><a href="#plr_packages_versions">Note on R Package Versions &amp; Dependencies</a></li>
</ul>
</li>
<li>
<a href="#bestpractices">Usage &amp; Best Practices</a>

<ul>
<li><a href="#makeplan">Make a Plan</a></li>
<li><a href="#dataprep">Data Preparation</a></li>
<li><a href="#returntypes">Return types</a></li>
<li><a href="#udf">PL/R UDF Definition</a></li>
<li><a href="#execution">PL/R Execution</a></li>
<li><a href="#persistence">Persisting R Models in the Database</a></li>
<li>
<a href="#parallelization">Verify Parallelization</a>

<ul>
<li><a href="#plr_parallelization_hostnames">Option 1: Via Segment Hostnames</a></li>
<li><a href="#plr_parallelization_timing">Option 2: Via Timing</a></li>
<li><a href="#plr_parallelization_cc">Option 3: Via Pivotal Command Center</a></li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#plr_details">More Details</a>

<ul>
<li>
<a href="#datatypes">Data Types</a>

<ul>
<li><a href="#plr_datatypes_input">PL/R Input Conversion: SQL Data Types → R Data Types</a></li>
<li><a href="#plr_datatypes_output">PL/R Output Conversion: R Data Types → SQL Data Types</a></li>
</ul>
</li>
<li><a href="#memory">Memory Limits</a></li>
</ul>
</li>
<li>
<a href="#plrexercises">Exercises</a>

<ul>
<li><a href="#plrexercises">PL/R Exercises</a></li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#rpostgresql">RPostgreSQL on Greenplum &amp; HAWQ</a>

<ul>
<li><a href="#rpostgresql">Introduction</a></li>
<li><a href="#rpostgresql_local">Local Development</a></li>
<li><a href="#plotting">Plotting</a></li>
<li><a href="#rpostgresql_plrcaveats">Caveats Around Usage Within PL/R</a></li>
</ul>
</li>
<li>
<a href="#pivotalr">PivotalR on Greenplum &amp; HAWQ</a>

<ul>
<li><a href="#pivotalr">Introduction</a></li>
<li><a href="#pivotalr_design">Design &amp; Features</a></li>
<li><a href="#pivotalr_demo">Demo</a></li>
<li><a href="#pivotalr_install">Download &amp; Installation</a></li>
</ul>
</li>
<li><a href="#shiny_cf">Shiny Apps on Cloud Foundry</a></li>
</ul>

<h1>
<a id="-overview" class="anchor" href="#-overview" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="overview"></a> Overview</h1>

<p>In a traditional analytics workflow using R, data are loaded from a data source, modeled or visualized, and the model scoring results are pushed back to the data source. Such an approach works well when (i) the amount of data can be loaded into memory, and (ii) the transfer of large amounts of data is inexpensive and/or fast.  One of the major focus areas of this guide is to explore the situation involving large data sets where these two assumptions are violated. </p>

<p><a href="http://pivotal.io/big-data/pivotal-greenplum">Greenplum Database (GPDB)</a> and <a href="http://hawq.incubator.apache.org">Apache HAWQ</a> offer several alternatives to interact with R using the in-database/in-Hadoop analytics paradigm. There are many ways to use R with these platforms. In this guide, we will outline the most common practices and provide code examples to help get you started.</p>

<p>Regardless of the size of data, the "last mile" of operationalizing data-driven discoveries has traditionally often been an area of challenge.  With the advent of lightweight web frameworks for data scientists such as <a href="http://shiny.rstudio.com/">Shiny</a> and highly automated hosting platforms such as <a href="https://www.cloudfoundry.org/">Cloud Foundry (CF)</a>, the effort involved in developing data-driven smart apps for end users has been reduced vastly.  In this document, we will also outline some guidelines to help you get started on pushing your Shiny apps to the cloud.  </p>

<p>Official documentation can be found here:</p>

<ul>
<li>GPDB

<ul>
<li><a href="http://pivotal.io/big-data/pivotal-greenplum">Product Page</a></li>
<li><a href="http://gpdb.docs.pivotal.io/index.html">Documentation</a></li>
<li><a href="http://gpdb.docs.pivotal.io/4320/pdf/GPDB43InstallGuide.pdf">Installation guide</a></li>
<li><a href="http://gpdb.docs.pivotal.io/4320/pdf/GPDB43AdminGuide.pdf">Administrator guide</a></li>
</ul>
</li>
<li>Apache HAWQ

<ul>
<li><a href="http://hawq.incubator.apache.org">Product Page</a></li>
<li><a href="http://hawq.docs.pivotal.io/index.html">Documentation</a></li>
<li><a href="http://hawq.docs.pivotal.io/docs-hawq/topics/HAWQInstallationandUpgrade.html">Installation guide</a></li>
<li><a href="http://hawq.docs.pivotal.io/docs-hawq/topics/HAWQAdministration.html">Administrator guide</a></li>
<li>
<a href="https://github.com/apache/incubator-hawq">Github Repository</a> </li>
</ul>
</li>
<li>Cloud Foundry

<ul>
<li><a href="https://www.cloudfoundry.org/">Product Page</a></li>
<li><a href="http://docs.cloudfoundry.org/">Documentation</a></li>
<li><a href="https://docs.pivotal.io/pcf-dev/">Installing PCF-Dev (i.e. CF in a VM)</a></li>
<li>
<a href="https://github.com/cloudfoundry">Github Repository</a> </li>
<li><a href="https://github.com/wjjung317/heroku-buildpack-r">R Buildpack</a></li>
</ul>
</li>
</ul>

<p>Note that this Github page is intended as a guide for <strong>practitioners</strong> and <strong>should not</strong> be considered official documentation. The intention is to give pragmatic tips on how to use the GPDB, HAWQ, and Cloud Foundry with the R statistical programming environment.  </p>

<h1>
<a id="-plr-on-greenplum--hawq" class="anchor" href="#-plr-on-greenplum--hawq" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr"></a> PL/R on Greenplum &amp; HAWQ</h1>

<h2>
<a id="-getting-started" class="anchor" href="#-getting-started" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_gettingstarted"></a> Getting Started</h2>

<h3>
<a id="-plr-architecture" class="anchor" href="#-plr-architecture" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_arch"></a> PL/R Architecture</h3>

<p><img src="https://github.com/zimmeee/gp-r/blob/master/figures/PLR_GPDB_Architecture.png?raw=true" alt="alt text" title="Distributed PL/R architecture on GPDB"></p>

<p>PL/R provides a connection from the database to R -- which is running on every segment of the Greenplum instance -- to allow you to write procedural functions in R. In this setup R is not a client application that runs on the desktop like pgadmin. It runs on each segment of the server.</p>

<h3>
<a id="datasets-for-examples" class="anchor" href="#datasets-for-examples" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Datasets for Examples</h3>

<p>This guide contains code examples interspersed with explanations in natural language. You are encouraged to follow along with the examples, most of which will use the <code>abalone</code> <a href="http://archive.ics.uci.edu/ml/datasets/Abalone">dataset</a> from the UC Irvine <a href="http://archive.ics.uci.edu/ml/index.html">Machine Learning Repository</a>.</p>

<p>To get started, download the data onto the file system of the GPDB/HAWQ host machine, and note the path: </p>

<pre><code>wget http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data
pwd
</code></pre>

<p>Next, create a table in GPDB/HAWQ to store the abalone data. Note that <code>/path/to/data</code> is the path returned by <code>pwd</code> in the previous line of code. </p>

<pre><code>DROP TABLE IF EXISTS abalone;
CREATE TABLE abalone (sex text, length float8, diameter float8, height float8, whole_weight float8, shucked_weight float8, viscera_weight float8, shell_weight float8, rings float8) 
DISTRIBUTED RANDOMLY;
COPY abalone FROM '/path/to/data/abalone.data' WITH CSV;
</code></pre>

<p>You should now have a table in the <code>public</code> schema of your database containing 4177 rows.</p>

<pre><code>user# select count(*) from abalone;
 count 
-------
  4177
(1 row)
</code></pre>

<h3>
<a id="-installation" class="anchor" href="#-installation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="installation"></a> Installation</h3>

<h4>
<a id="install-and-verify-plr" class="anchor" href="#install-and-verify-plr" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install and verify PL/R</h4>

<p>Pivotal Engineering ships its own version of PL/R as a gppkg. You will not be able to download the source from Joe Conway's website
and compile it against the postgres headers supplied by Pivotal. Although HAWQ and Greenplum are based on Postgres 8.2, the source codes have diverged enough that your compilation of PL/R source (for Postgres 8.2) with Pivotal-supplied postgres headers will not be successful.
Please contact support to obtain the gppkg for PL/R for your installation (internally, it can also be downloaded from Pivotal Network). Once obtained, the gppkg for PL/R can be installed by following the steps below:</p>

<p>Gppkg command can be used to install PL/R on all segments.</p>

<pre><code>gppkg --install plr-1.0-rhel5-x86_64.gppkg
</code></pre>

<p>This will install both PL/R and R as well.
You will find a folder <code>/usr/local/greenplum-db/ext/R-2.13/</code> upon the successful installation of the previous command.</p>

<p>You should see a trace like the following for each segment</p>

<pre><code>bash-4.1$ gppkg --install plr-1.0-rhel5-x86_64.gppkg
20130524:10:56:17:007456 gppkg:agni_centos:gpadmin-[INFO]:-Starting gppkg with args: --install plr-1.0-rhel5-x86_64.gppkg
20130524:10:56:18:007456 gppkg:agni_centos:gpadmin-[INFO]:-Installing package plr-1.0-rhel5-x86_64.gppkg
20130524:10:56:18:007456 gppkg:agni_centos:gpadmin-[INFO]:-Validating rpm installation cmdStr='rpm --test -i /usr/local/greenplum-db-4.2.2.4/.tmp/plr-1.0-1.x86_64.rpm /usr/local/greenplum-db-4.2.2.4/.tmp/R-2.13.0-1.x86_64.rpm --dbpath /usr/local/greenplum-db-4.2.2.4/share/packages/database --prefix /usr/local/greenplum-db-4.2.2.4'
20130524:10:56:18:007456 gppkg:agni_centos:gpadmin-[INFO]:-Installing plr-1.0-rhel5-x86_64.gppkg locally
20130524:10:56:19:007456 gppkg:agni_centos:gpadmin-[INFO]:-Validating rpm installation cmdStr='rpm --test -i /usr/local/greenplum-db-4.2.2.4/.tmp/plr-1.0-1.x86_64.rpm /usr/local/greenplum-db-4.2.2.4/.tmp/R-2.13.0-1.x86_64.rpm --dbpath /usr/local/greenplum-db-4.2.2.4/share/packages/database --prefix /usr/local/greenplum-db-4.2.2.4'
20130524:10:56:19:007456 gppkg:agni_centos:gpadmin-[INFO]:-Installing rpms cmdStr='rpm -i /usr/local/greenplum-db-4.2.2.4/.tmp/plr-1.0-1.x86_64.rpm /usr/local/greenplum-db-4.2.2.4/.tmp/R-2.13.0-1.x86_64.rpm --dbpath /usr/local/greenplum-db-4.2.2.4/share/packages/database --prefix=/usr/local/greenplum-db-4.2.2.4'
20130524:10:56:20:007456 gppkg:agni_centos:gpadmin-[INFO]:-Completed local installation of plr-1.0-rhel5-x86_64.gppkg.
20130524:10:56:20:007456 gppkg:agni_centos:gpadmin-[INFO]:-Please source your $GPHOME/greenplum_path.sh file and restart the database.
You can enable PL/R by running createlang plr -d mydatabase.
20130524:10:56:20:007456 gppkg:agni_centos:gpadmin-[INFO]:-plr-1.0-rhel5-x86_64.gppkg successfully installed.
</code></pre>

<p>The installation can be verified by checking for the existence of the PL/R shared object in <code>/usr/local/greenplum-db/lib/postgresql/plr.so</code></p>

<p>Now you'll have to source /usr/local/greenplum-db/greenplum_path.sh and restart GPDB for changes to the <code>LD_LIBRARY_PATH</code> environment variable to take effect.
Following the installation you'll see that the environment variable <code>R_HOME</code> has been set on all segments.</p>

<pre><code>[gpadmin@mdw ~]$ echo $R_HOME
/usr/local/greenplum-db/./ext/R-2.13.0/lib64/R
[gpadmin@mdw ~]$ 
</code></pre>

<p>You can then install PL/R on your database by running</p>

<pre><code>CREATE LANGUAGE PLR;
</code></pre>

<p>You may also install it on the template1 database to ensure every newly created database automatically has PL/R installed in it.</p>

<h3>
<a id="-note-on-permissions" class="anchor" href="#-note-on-permissions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="permissions"></a> Note on Permissions</h3>

<p>R is an <a href="http://www.postgresql.org/docs/current/interactive/catalog-pg-language.html">untrusted language</a>. Only superusers can create functions in untrusted languages. A discussion as to whether granting super user privileges on the database is acceptable needs to be an explicit step in selecting PL/R for your analytics project. </p>

<p>This is what happens when you try to create a PL/R function when you aren't a superuser:</p>

<pre><code>ERROR:  permission denied for language plr

********** Error **********

ERROR: permission denied for language plr
SQL state: 42501
</code></pre>

<p>You do not need superuser privileges to EXECUTE a PL/R function, only to CREATE a PL/R function. Thus, non-superusers <em>can run</em> a PL/R function that was created by a superuser. In the GP Admin Guide there is a section entitled 'Managing Object Privileges' which outlines how to grant privileges to other roles for executing untrusted languages. </p>

<p>GRANT USAGE privilege to the account 
<a href="http://lists.pgfoundry.org/pipermail/plr-general/2010-August/000441.html">http://lists.pgfoundry.org/pipermail/plr-general/2010-August/000441.html</a></p>

<h2>
<a id="-leveraging-r-packages" class="anchor" href="#-leveraging-r-packages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="packages"></a> Leveraging R Packages</h2>

<p>The trick to installing R packages in a distributed Greenplum environment is that each segment has it's own R instance running and thus each segment needs its own version of all of the required packages. At a high-level, the steps for installing R packages on a Greenplum instance are:</p>

<ol>
<li>Get the package tars from CRAN (<code>wget</code>)</li>
<li>Copy the tar to all the segments on the DCA (<code>gpscp</code>)</li>
<li>Install the package (<code>gpssh</code>, then <code>R CMD INSTALL</code>)</li>
</ol>

<p>Note that any time you install a new R library/package using:</p>

<pre><code>R CMD INSTALL &lt;package name&gt;
</code></pre>

<p>The resulting shared object (.so file) of the library
should be generated in <code>/usr/local/greenplum-db/ext/R-2.13.0/lib64/R/library/&lt;library_name&gt;</code></p>

<h3>
<a id="-checking-r-package-availability" class="anchor" href="#-checking-r-package-availability" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_packages_check"></a> Checking R Package Availability</h3>

<p>R packages are the special sauce of R. This section explains how to check whether a package is installed and how to install new packages. The simplest way to check if the requires R packages are available for PL/R is to <code>gpssh</code> into all the nodes and test if you are able to find the version of the required package. All the nodes
should return the correct version of the package, if the installation was successful.</p>

<pre><code>gpssh -f all_hosts
=&gt; echo "packageVersion('rpart')" | R --no-save

[sdw11] &gt; packageVersion('rpart')
[sdw11] [1] ‘3.1.49’
[ sdw9] &gt; packageVersion('rpart')
[ sdw9] [1] ‘3.1.49’
.
.
.
</code></pre>

<p>If the package is unavailable, the above code will error out. In the snippet below, we check for the version of the <code>HMM</code> package
in our installation. As there is no such package installed, the command will not execute successfully.</p>

<pre><code>gpssh -f all_hosts
=&gt; echo "packageVersion('hmm')" | R --no-save
[ sdw2] &gt; packageVersion('hmm')
[ sdw2] Error in packageVersion("hmm") : package ‘hmm’ not found
[ sdw2] Execution halted
[ sdw3] &gt; packageVersion('hmm')
[ sdw3] Error in packageVersion("hmm") : package ‘hmm’ not found
[ sdw3] Execution halted

</code></pre>

<p>If you do not have access to SSH into the GPDB or you prefer to only deal with UDFs to tell you if a PL/R package is present or absent, then you can write UDFs like the following:</p>

<p>A simple test if a package can be loaded can be done by this function:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">CREATE OR REPLACE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">R_test_require</span>(fname <span class="pl-k">text</span>)
RETURNS <span class="pl-k">boolean</span> <span class="pl-k">AS</span>
$BODY$
    return(require(fname,<span class="pl-c1">character</span>.<span class="pl-c1">only</span><span class="pl-k">=</span>T))
$BODY$
LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;</pre></div>

<p>If you want to check for a package called 'rpart', you would do</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span> R_test_require(<span class="pl-s"><span class="pl-pds">'</span>rpart<span class="pl-pds">'</span></span>);</pre></div>

<p>And it will return <code>TRUE</code> if the package could be loaded and <code>FALSE</code> if it couldn't. However, this only works on the node that you are currently logged on to.</p>

<p>To test the R installations on all nodes you would first create a dummy table with a series of integers that will be stored on different nodes in GPDB, like this:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">DROP</span> <span class="pl-k">TABLE</span> IF EXISTS simple_series;
<span class="pl-k">CREATE</span> <span class="pl-k">TABLE</span> <span class="pl-en">simple_series</span> <span class="pl-k">AS</span> (<span class="pl-k">SELECT</span> generate_series(<span class="pl-c1">0</span>,<span class="pl-c1">1000</span>) <span class="pl-k">AS</span> id);</pre></div>

<p>Also, since we want to know which host we are on we create a function to tell us:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">CREATE OR REPLACE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">R_return_host</span>()
RETURNS <span class="pl-k">text</span> <span class="pl-k">AS</span>
$BODY$
  return(system(<span class="pl-s"><span class="pl-pds">"</span>hostname<span class="pl-pds">"</span></span>,intern<span class="pl-k">=</span>T))
$BODY$
LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;</pre></div>

<p>Now we can check for each id (ids are stored on different nodes) if rpart is installed like this:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">DROP</span> <span class="pl-k">TABLE</span> IF EXISTS result_nodes;
<span class="pl-k">CREATE</span> <span class="pl-k">TABLE</span> <span class="pl-en">result_nodes</span> <span class="pl-k">AS</span> 
    (<span class="pl-k">SELECT</span> id, R_return_host() <span class="pl-k">AS</span> hostname, R_test_require(<span class="pl-s"><span class="pl-pds">'</span>rpart<span class="pl-pds">'</span></span>) <span class="pl-k">AS</span> result 
    <span class="pl-k">FROM</span> simple_series <span class="pl-k">group by</span> id); </pre></div>

<p><code>result_nodes</code> is a table that contains for every id, the host that it is stored on as <code>hostname</code>, and the result of <code>R_test_require</code> as result. Since we only want to know for every host once, we group by <code>hostname</code> like this:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">select</span> hostname, bool_and(result) <span class="pl-k">AS</span> host_result 
<span class="pl-k">FROM</span> result_nodes 
<span class="pl-k">GROUP BY</span> hostname 
<span class="pl-k">ORDER BY</span> hostname;</pre></div>

<p>For a hostname where <code>R_test_require</code> returned true for all ids, the value in the column <code>host_result</code> will be true. If on a certain host the package couldn't be loaded, <code>host_result</code> will be false.</p>

<h3>
<a id="-installing-r-packages" class="anchor" href="#-installing-r-packages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_packages_install"></a> Installing R Packages</h3>

<p>Before installing the packages for PL/R ensure that you are referring to the right R binary in your PATH and also ensure that the environment variable <code>R_HOME</code> is referring to the right location where you installed R. These paths should be identical on all master and segment nodes.</p>

<p>Some users have a separate stand-alone installation of R on just the master node. If this is the case with your installation, ensure that this does not conflict with installation you need for PL/R to run on multiple segments.</p>

<p>For a given R package, identify all dependent R packages and the package URLs.  This can be found by selecting the given package from the following navigation page: 
<code>http://cran.r-project.org/web/packages/available_packages_by_name.html</code></p>

<p>From the page for the <code>arm</code> library, it can be seen that this library requires the following R libraries: <code>Matrix</code>, <code>lattice</code>, <code>lme4</code>, <code>R2WinBUGS</code>, <code>coda</code>, <code>abind</code>, <code>foreign</code>, <code>MASS</code></p>

<p>From the command line, use wget to download the required packages' <code>tar.gz</code> files to the master node:</p>

<pre><code>wget http://cran.r-project.org/src/contrib/arm_1.5-03.tar.gz
wget http://cran.r-project.org/src/contrib/Archive/Matrix/Matrix_1.0-1.tar.gz
wget http://cran.r-project.org/src/contrib/Archive/lattice/lattice_0.19-33.tar.gz
wget http://cran.r-project.org/src/contrib/lme4_0.999375-42.tar.gz
wget http://cran.r-project.org/src/contrib/R2WinBUGS_2.1-18.tar.gz
wget http://cran.r-project.org/src/contrib/coda_0.14-7.tar.gz
wget http://cran.r-project.org/src/contrib/abind_1.4-0.tar.gz
wget http://cran.r-project.org/src/contrib/foreign_0.8-49.tar.gz
wget http://cran.r-project.org/src/contrib/MASS_7.3-17.tar.gz
</code></pre>

<p>Using <code>gpscp</code> and the hostname file, copy the <code>tar.gz</code> files to the same directory on all nodes of the GPDB/HAWQ cluster.  Note that this may require root access. (note: location and name of host file may be different. On our DCA its /home/gpadmin/all_hosts)</p>

<pre><code>gpscp -f /home/gpadmin/all_hosts lattice_0.19-33.tar.gz =:/home/gpadmin 
gpscp -f /home/gpadmin/all_hosts Matrix_1.0-1.tar.gz =:/home/gpadmin 
gpscp -f /home/gpadmin/all_hosts abind_1.4-0.tar.gz =:/home/gpadmin 
gpscp -f /home/gpadmin/all_hosts coda_0.14-7.tar.gz =:/home/gpadmin 
gpscp -f /home/gpadmin/all_hosts R2WinBUGS_2.1-18.tar.gz =:/home/gpadmin 
gpscp -f /home/gpadmin/all_hosts lme4_0.999375-42.tar.gz =:/home/gpadmin 
gpscp -f /home/gpadmin/all_hosts MASS_7.3-17.tar.gz =:/home/gpadmin
gpscp -f /home/gpadmin/all_hosts arm_1.5-03.tar.gz =:/home/gpadmin
</code></pre>

<p><code>gpssh</code> into all segments (<code>gpssh -f /home/gpadmin/all_hosts</code>).  Install the packages from the command prompt using the <code>R CMD INSTALL</code> command.  Note that this may require root access </p>

<pre><code>R CMD INSTALL lattice_0.19-33.tar.gz Matrix_1.0-1.tar.gz abind_1.4-0.tar.gz coda_0.14-7.tar.gz R2WinBUGS_2.1-18.tar.gz lme4_0.999375-42.tar.gz MASS_7.3-17.tar.gz arm_1.5-03.tar.gz
</code></pre>

<p>Check that the newly installed package is listed under the <code>$R_HOME/library</code> directory on all the segments (convenient to use <code>gpssh</code> here as well).</p>

<h3>
<a id="-note-on-r-package-versions--dependencies" class="anchor" href="#-note-on-r-package-versions--dependencies" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_packages_versions"></a> Note on R Package Versions &amp; Dependencies</h3>

<p>Sometimes the current version of a package has dependencies on an earlier version of R. If this happens, you might get an error message like:</p>

<pre><code>In getDependencies(pkgs, dependencies, available, lib) :
  package ‘matrix’ is not available (for R version 2.13.0)
</code></pre>

<p>Fortunately, there are older versions of most packages available in the CRAN archive. One heuristic we’ve found useful is to look at the release date of the R version installed on the machine. At the time of writing, it is v2.13 on our analytics DCA, which was released on 13-Apr-2011 (<a href="http://cran.r-project.org/src/base/R-2/">http://cran.r-project.org/src/base/R-2/</a>). Armed with this date, go to the archive folder for the package you are installing and find the version that was released immediately prior to that date. For instance, the v1.5.3 of the package <code>glmnet</code> was released on 01-Mar-2011 and should be compatible with R v2.13 (<a href="http://cran.r-project.org/src/contrib/Archive/glmnet/">http://cran.r-project.org/src/contrib/Archive/glmnet/</a> ) and download that version. This manual heuristic works reasonably well for finding compatible package versions. </p>

<h2>
<a id="-usage--best-practices" class="anchor" href="#-usage--best-practices" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="bestpractices"></a> Usage &amp; Best Practices</h2>

<p>Here we outline workflows that have worked well for us in past experiences using R on GPDB &amp; HAWQ.  </p>

<p>One overarching theme for PL/R on GPDB/HAWQ is that it is best suited in scenarios where the problem that you want to solve is one that is embarrassingly parallelizable. A simple way to think about PL/R is that it is provides functionality akin to MapReduce or R’s apply family of functions – with the added bonus of leveraging GPDB/HAWQ native architecture to execute each mapper. In other words, it provides a nice framework for you to run parallelized <code>for</code> loops containing R jobs in GPDB/HAWQ.  We focus our description of best practices around this theme.</p>

<ul>
<li><a href="#makeplan">Make a plan</a></li>
<li><a href="#dataprep">Data prep</a></li>
<li><a href="#returntypes">Return types</a></li>
<li><a href="#udf">PL/R UDF Definition</a></li>
<li><a href="#execution">PL/R Execution</a></li>
<li><a href="#persistence">Persisting R Models in GPDB &amp; HAWQ</a></li>
<li><a href="#parallelization">Verifying Parallelization</a></li>
</ul>

<h3>
<a id="-make-a-plan" class="anchor" href="#-make-a-plan" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="makeplan"></a> Make a Plan</h3>

<p>Before doing anything, ask yourself whether the problem you are solving is explicitly parallelizable.  If so, identify what you’d like to parallelize by.  In other words, what is the index of your for loop?  This will play a large role in determining how you will prepare your data and build your PL/R function.</p>

<p>Using the abalone data as an example, let’s suppose you were interested in building a separate, completely independent model for each sex of abalone in the dataset.  Under this scenario, it’s clear that it would then make sense to parallelize by the abalone’s sex.  </p>

<h3>
<a id="-data-preparation" class="anchor" href="#-data-preparation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="dataprep"></a> Data Preparation</h3>

<p>It’s often good practice to build another version of your table, dimensioned by the field by which you’d like to parallelize.  Let’s call this field the parallelization index for shorthand.  You essentially want to build a table where each row contains all the data for each value of the parallelization index.  This is done by array aggregation.  Using the SQL <code>array_agg()</code> function, aggregate all of the records for each unique value of the parallelization index into a single row.  </p>

<p>An example will make this more clear.  Let’s take a look at our raw abalone table:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> abalone <span class="pl-k">LIMIT</span> <span class="pl-c1">3</span>;
 sex | length | diameter | height | whole_weight | shucked_weight | viscera_weight | shell_weight | rings 
<span class="pl-c">-----+--------+----------+--------+--------------+----------------+----------------+--------------+-------</span>
 M   |  <span class="pl-c1">0</span>.<span class="pl-c1">405</span> |     <span class="pl-c1">0</span>.<span class="pl-c1">31</span> |    <span class="pl-c1">0</span>.<span class="pl-c1">1</span> |        <span class="pl-c1">0</span>.<span class="pl-c1">385</span> |          <span class="pl-c1">0</span>.<span class="pl-c1">173</span> |         <span class="pl-c1">0</span>.<span class="pl-c1">0915</span> |         <span class="pl-c1">0</span>.<span class="pl-c1">11</span> |     <span class="pl-c1">7</span>
 M   |  <span class="pl-c1">0</span>.<span class="pl-c1">425</span> |     <span class="pl-c1">0</span>.<span class="pl-c1">35</span> |  <span class="pl-c1">0</span>.<span class="pl-c1">105</span> |        <span class="pl-c1">0</span>.<span class="pl-c1">393</span> |           <span class="pl-c1">0</span>.<span class="pl-c1">13</span> |          <span class="pl-c1">0</span>.<span class="pl-c1">063</span> |        <span class="pl-c1">0</span>.<span class="pl-c1">165</span> |     <span class="pl-c1">9</span>
 I   |  <span class="pl-c1">0</span>.<span class="pl-c1">315</span> |    <span class="pl-c1">0</span>.<span class="pl-c1">245</span> |  <span class="pl-c1">0</span>.<span class="pl-c1">085</span> |       <span class="pl-c1">0</span>.<span class="pl-c1">1435</span> |          <span class="pl-c1">0</span>.<span class="pl-c1">053</span> |         <span class="pl-c1">0</span>.<span class="pl-c1">0475</span> |         <span class="pl-c1">0</span>.<span class="pl-c1">05</span> |     <span class="pl-c1">8</span>
(<span class="pl-c1">3</span> rows)</pre></div>

<p>Let’s suppose that the end goal is to build a separate regression model for each sex with shucked_weight as the response variable and rings, diameter as explanatory variables.  Thinking ahead to this end goal, you would then create another version of the data table by:</p>

<ol>
<li>Array aggregating each variable of interest,</li>
<li>Grouping by the parallelization index, and</li>
<li>Distributing by the parallelization index</li>
</ol>

<p>To continue our example:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">DROP</span> <span class="pl-k">TABLE</span> IF EXISTS abalone_array;
<span class="pl-k">CREATE</span> <span class="pl-k">TABLE</span> <span class="pl-en">abalone_array</span> <span class="pl-k">AS</span> <span class="pl-k">SELECT</span> 
sex::<span class="pl-k">text</span>
, array_agg(shucked_weight::float8) <span class="pl-k">as</span> s_weight
, array_agg(rings::float8) <span class="pl-k">as</span> rings
, array_agg(diameter::float8) <span class="pl-k">as</span> diameter 
<span class="pl-k">FROM</span> abalone 
<span class="pl-k">GROUP BY</span> sex 
DISTRIBUTED BY (sex);</pre></div>

<p>The raw table is array aggregated into a table with rows equal to the number of unique values of the parallelization index.  For this specific example, there are three unique values of sex in the abalone data, and thus there are three rows in the abalone_array table.   </p>

<h3>
<a id="-return-types" class="anchor" href="#-return-types" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="returntypes"></a> Return Types</h3>

<p>As described in the Data Types section, it’s often difficult to read SQL arrays, and it's not possible to have SQL arrays containing both text and numeric entries.  For this reason, our best practice is to use custom composite types as return types for PL/R functions in Greenplum.  </p>

<p>It’s useful to think ahead and identify what the final output of your PL/R function will be.  In the case of our example, since we are running regressions, let’s suppose we want to return information that looks a lot like R’s <code>summary.lm()</code> function.  In particular, we are interested in getting back a table with each explanatory variable’s name, the coefficient estimate, standard error, t-statistic, and p-value.  With this in mind, we build a custom composite type as a template for the output we intend to get back from our PL/R function.  </p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">DROP</span> <span class="pl-k">TYPE</span> IF EXISTS lm_abalone_type CASCADE;
<span class="pl-k">CREATE</span> <span class="pl-k">TYPE</span> <span class="pl-en">lm_abalone_type</span> <span class="pl-k">AS</span> (
Variable <span class="pl-k">text</span>, Coef_Est float, Std_Error float, T_Stat float, P_Value float); </pre></div>

<h3>
<a id="-plr-udf-definition" class="anchor" href="#-plr-udf-definition" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="udf"></a> PL/R UDF Definition</h3>

<p>Now that we’ve defined the structure of our input and output values, we can go ahead and tell GPDB/HAWQ and R what we want to do with this data.  We are now ready to define our PL/R function. </p>

<p>A couple of helpful rules to follow here:</p>

<ul>
<li>Each argument of the PL/R function and its specified data type should correspond to a column that exists in the array aggregated table that was created in the Data Prep step</li>
<li>The return data type of the PL/R function should be a SETOF the composite type that was created in the Return Types step</li>
</ul>

<p>Continuing our example using the abalone data, we define the following PL/R function:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">CREATE OR REPLACE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">lm_abalone_plr</span>(s_weight float8[], rings float8[], diameter float8[]) 
RETURNS SETOF lm_abalone_type <span class="pl-k">AS</span> 
$$ 
    m1<span class="pl-k">&lt;</span><span class="pl-k">-</span> lm(s_weight~rings<span class="pl-k">+</span>diameter)
    m1_s<span class="pl-k">&lt;</span><span class="pl-k">-</span> summary(m1)$coef
    temp_m1<span class="pl-k">&lt;</span><span class="pl-k">-</span> <span class="pl-c1">data</span>.<span class="pl-c1">frame</span>(rownames(m1_s), m1_s)
    return(temp_m1)
$$ 
LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;</pre></div>

<h3>
<a id="-plr-execution" class="anchor" href="#-plr-execution" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="execution"></a> PL/R Execution</h3>

<p>We then execute the PL/R function by specifying the parallelization index and the function call in the SELECT statement.  </p>

<p>To conclude our example, we run the following SELECT statement to run 3 separate regression models; one model for each sex.  Under this scenario, execution is parallelized by the abalone’s sex:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span>  sex, (lm_abalone_plr(s_weight,rings,diameter)).<span class="pl-k">*</span> <span class="pl-k">FROM</span> abalone_array;
 sex |  variable   |       coef_est       |      std_error       |       t_stat       |        p_value        
 <span class="pl-c">-----+-------------+----------------------+----------------------+--------------------+----------------------- </span>
 F   | (Intercept) |   <span class="pl-k">-</span><span class="pl-c1">0</span>.<span class="pl-c1">617050922097655</span> |   <span class="pl-c1">0</span>.<span class="pl-c1">0169416168113397</span> |   <span class="pl-k">-</span><span class="pl-c1">36</span>.<span class="pl-c1">422198009144</span> | <span class="pl-c1">6</span>.03016903934925e<span class="pl-k">-</span><span class="pl-c1">201</span> 
 F   | rings       | <span class="pl-k">-</span><span class="pl-c1">0</span>.<span class="pl-c1">00956233525043721</span> | <span class="pl-c1">0</span>.<span class="pl-c1">000835808125948978</span> |  <span class="pl-k">-</span><span class="pl-c1">11</span>.<span class="pl-c1">4408258947951</span> |  <span class="pl-c1">5</span>.96598342834597e<span class="pl-k">-</span><span class="pl-c1">29</span> 
 F   | diameter    |     <span class="pl-c1">2</span>.<span class="pl-c1">57219713416591</span> |   <span class="pl-c1">0</span>.<span class="pl-c1">0365667203043869</span> |    <span class="pl-c1">70</span>.<span class="pl-c1">342571407951</span> |                     <span class="pl-c1">0</span> 
 M   | (Intercept) |   <span class="pl-k">-</span><span class="pl-c1">0</span>.<span class="pl-c1">534293488484019</span> |   <span class="pl-c1">0</span>.<span class="pl-c1">0148876715438078</span> |  <span class="pl-k">-</span><span class="pl-c1">35</span>.<span class="pl-c1">8883178549332</span> | <span class="pl-c1">5</span>.42293200035969e<span class="pl-k">-</span><span class="pl-c1">205</span> 
 M   | rings       |  <span class="pl-k">-</span><span class="pl-c1">0</span>.<span class="pl-c1">0101856670676353</span> |  <span class="pl-c1">0</span>.<span class="pl-c1">00096233015174409</span> |  <span class="pl-k">-</span><span class="pl-c1">10</span>.<span class="pl-c1">5843790191704</span> |  <span class="pl-c1">2</span>.59455668009866e<span class="pl-k">-</span><span class="pl-c1">25</span> 
 M   | diameter    |     <span class="pl-c1">2</span>.<span class="pl-c1">45006792350753</span> |   <span class="pl-c1">0</span>.<span class="pl-c1">0345072752341834</span> |   <span class="pl-c1">71</span>.<span class="pl-c1">0014890158715</span> |                     <span class="pl-c1">0</span>
 I   | (Intercept) |   <span class="pl-k">-</span><span class="pl-c1">0</span>.<span class="pl-c1">236131314300337</span> |  <span class="pl-c1">0</span>.<span class="pl-c1">00601596875673268</span> |  <span class="pl-k">-</span><span class="pl-c1">39</span>.<span class="pl-c1">2507547576729</span> | <span class="pl-c1">6</span>.73787958361764e<span class="pl-k">-</span><span class="pl-c1">225</span>
 I   | rings       | <span class="pl-k">-</span><span class="pl-c1">0</span>.<span class="pl-c1">00046870969168018</span> | <span class="pl-c1">0</span>.<span class="pl-c1">000850383676525165</span> | <span class="pl-k">-</span><span class="pl-c1">0</span>.<span class="pl-c1">551174375307179</span> |     <span class="pl-c1">0</span>.<span class="pl-c1">581606099530735</span>
 I   | diameter    |     <span class="pl-c1">1</span>.<span class="pl-c1">31967087153234</span> |   <span class="pl-c1">0</span>.<span class="pl-c1">0242402717496186</span> |   <span class="pl-c1">54</span>.<span class="pl-c1">4412573078149</span> |                     <span class="pl-c1">0</span>
(<span class="pl-c1">9</span> rows)
</pre></div>

<h3>
<a id="-persisting-r-models-in-the-database" class="anchor" href="#-persisting-r-models-in-the-database" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="persistence"></a> Persisting R Models in the Database</h3>

<p>One benefit of using PL/R on a parallelized platform like GPDB/HAWQ is the ability to perform scoring in parallel across all the segments.
If you've trained a GLM model for instance, you could save a serialized version of this model in a database table and de-serialize it when needed and use it for scoring.</p>

<p>Typically the models are built once or are trained periodically depending on what the application may be, but the scoring may have to happen in real-time as new data becomes available.
If the data to be scored is stored in a table distributed across the segments on GPDB/HAWQ, then by ensuring the trained models are also distributed across the same segments, we can achieve parallel scoring through PL/R.</p>

<p>The simplest approach would be to serialize the entire model into a byte array and store it in a table, although not all parameters of the R model are required for scoring. For example, for linear or logistic regression we only need the coefficients of the features to perform scoring. Advanced users should be able to extract only the relevant parameters from the model and serialize them into a byte array on a table. This will improve scoring speed as the segment nodes won't have to de-serialize large byte arrays. Another optimization that will speed up scoring will be to pre-load the models into memory on the segment nodes - so that models are not de-serialized for every PL/R function call. In both these cases the user will have to write additional logic beside the scoring itself, for the optimization.</p>

<p>In the sample code shown below we demonstrate some of these optimizations. This guide is work in progress and in the upcoming versions we will include more examples to optimize the scoring function.</p>

<p>First we'll define a custom record type to hold the results from a GLM model. This is equivalent to the summary() function in R.</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">DROP</span> <span class="pl-k">TYPE</span> IF EXISTS <span class="pl-c1">gpdemo</span>.<span class="pl-c1">glm_result_type</span> CASCADE;
    <span class="pl-k">CREATE</span> <span class="pl-k">TYPE</span> <span class="pl-en">gpdemo</span>.glm_result_type 
    <span class="pl-k">AS</span> 
    (
        params <span class="pl-k">text</span>, 
        estimate float, 
        std_Error float, 
        z_value float, 
        pr_gr_z float
    );</pre></div>

<p>Here is a PL/R function that demonstrates how a trained GLM model can be serialized as a byte array. The sample table <code>patient_history_train</code> is included in the data folder of this repository.</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">DROP</span> <span class="pl-k">FUNCTION</span> IF EXISTS <span class="pl-c1">gpdemo</span>.<span class="pl-c1">mdl_save_demo</span>();
    <span class="pl-k">CREATE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">gpdemo</span>.mdl_save_demo() 
        RETURNS <span class="pl-k">bytea</span> 
        <span class="pl-k">AS</span>
    $$
         <span class="pl-c">#Read the previously created patient_history training set</span>
         dataset <span class="pl-k">&lt;</span><span class="pl-k">-</span> <span class="pl-c1">pg</span>.<span class="pl-c1">spi</span>.exec(<span class="pl-s"><span class="pl-pds">'</span>select * from gpdemo.patient_history_train<span class="pl-pds">'</span></span>);

         <span class="pl-c"># Use the subset function to select a subset of the columns</span>
             <span class="pl-c"># Indices 2:6 are age, gender, race, marital status and bmi</span>
             <span class="pl-c"># Indices 14:20 are med_cond1 to med_cond7</span>
             <span class="pl-c"># Index 26 is the label 'infection cost'</span>
         ds <span class="pl-k">=</span> subset(dataset,<span class="pl-k">select</span><span class="pl-k">=</span>c(<span class="pl-c1">2</span>:<span class="pl-c1">6</span>,<span class="pl-c1">14</span>:<span class="pl-c1">20</span>, <span class="pl-c1">26</span>))

         <span class="pl-c">#Define text  columns to be factor types</span>
         <span class="pl-c">#These include gender, race, marital_status         </span>
         ds$gender <span class="pl-k">=</span> <span class="pl-k">as</span>.factor(ds$gender)
         ds$race <span class="pl-k">=</span> <span class="pl-k">as</span>.factor(ds$race)
         ds$marital_status <span class="pl-k">=</span> <span class="pl-k">as</span>.factor(ds$marital_status)

         <span class="pl-c">#Fit a GLM</span>
         mdl <span class="pl-k">=</span> glm(formula <span class="pl-k">=</span> infection_cost ~ age <span class="pl-k">+</span>
                  gender <span class="pl-k">+</span>
                  race <span class="pl-k">+</span>
                  marital_status <span class="pl-k">+</span>
                  bmi <span class="pl-k">+</span>
                  med_cond1 <span class="pl-k">+</span>
                  med_cond2 <span class="pl-k">+</span>
                  med_cond3 <span class="pl-k">+</span>
                  med_cond4 <span class="pl-k">+</span>
                  med_cond5 <span class="pl-k">+</span>
                  med_cond6 <span class="pl-k">+</span>
                  med_cond7 
            , family <span class="pl-k">=</span> gaussian, data<span class="pl-k">=</span>ds)
         <span class="pl-c">#The model is serialized and returned as a bytearray</span>
         return (serialize(mdl,<span class="pl-k">NULL</span>))
    $$
    LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;</pre></div>

<p>Here is a PL/R function to read a serialized PL/R model examine it's parameters.</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">DROP</span> <span class="pl-k">FUNCTION</span> IF EXISTS <span class="pl-c1">gpdemo</span>.<span class="pl-c1">mdl_load_demo</span>(<span class="pl-k">bytea</span>);
    <span class="pl-k">CREATE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">gpdemo</span>.mdl_load_demo(mdl <span class="pl-k">bytea</span>) 
        RETURNS setof <span class="pl-c1">gpdemo</span>.<span class="pl-c1">glm_result_type</span> 
        <span class="pl-k">AS</span>
    $$
         <span class="pl-c">#R-code goes here.</span>
         mdl <span class="pl-k">&lt;</span><span class="pl-k">-</span> unserialize(mdl)
         cf <span class="pl-k">&lt;</span><span class="pl-k">-</span> coef(summary(mdl))
         rows <span class="pl-k">=</span> dimnames(cf)[<span class="pl-c1">1</span>]
         <span class="pl-c">#Create a data frame and pass that as a result</span>
         result <span class="pl-k">=</span> <span class="pl-c1">data</span>.<span class="pl-c1">frame</span>(params<span class="pl-k">=</span>rows[[<span class="pl-c1">1</span>]],estimate<span class="pl-k">=</span>cf[,<span class="pl-c1">1</span>],error<span class="pl-k">=</span>cf[,<span class="pl-c1">2</span>],z_val<span class="pl-k">=</span>cf[,<span class="pl-c1">3</span>],pr_z<span class="pl-k">=</span>cf[,<span class="pl-c1">4</span>])
         return (result)
    $$
    LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;</pre></div>

<p>The function can be invoked like so:</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">select</span> (t).params, 
           (t).estimate,
           (t).std_Error, 
           (t).z_value float, 
           (t).pr_gr_z 
    <span class="pl-k">from</span> 
    (
           <span class="pl-c">-- The column 't' is of glm_result_type that we defined in step 3s.</span>
           <span class="pl-k">select</span> mdl_load_demo(model) <span class="pl-k">as</span> t 
           <span class="pl-k">from</span> mdls
    ) q ;</pre></div>

<p>Here is the PL/R function which demonstrate parallel scoring using the GLM model we trained in the example above.</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">DROP</span> <span class="pl-k">FUNCTION</span> IF EXISTS <span class="pl-c1">gpdemo</span>.<span class="pl-c1">mdl_score_demo</span>( <span class="pl-k">bytea</span>, 
                            <span class="pl-k">integer</span>,
                            <span class="pl-k">text</span>,
                            <span class="pl-k">text</span>,
                            <span class="pl-k">text</span>,
                            <span class="pl-k">double precision</span>,
                            <span class="pl-k">integer</span>,
                            <span class="pl-k">integer</span>,
                            <span class="pl-k">integer</span>,
                            <span class="pl-k">integer</span>,
                            <span class="pl-k">integer</span>,
                            <span class="pl-k">integer</span>,
                            <span class="pl-k">integer</span>
                              );
    <span class="pl-k">CREATE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">gpdemo</span>.mdl_score_demo( mdl <span class="pl-k">bytea</span>, 
                        age <span class="pl-k">integer</span>,
                        gender <span class="pl-k">text</span>,
                        race <span class="pl-k">text</span>,
                        marital_status <span class="pl-k">text</span>,
                        bmi <span class="pl-k">double precision</span>,
                        med_cond1 <span class="pl-k">integer</span>,
                        med_cond2 <span class="pl-k">integer</span>,
                        med_cond3 <span class="pl-k">integer</span>,
                        med_cond4 <span class="pl-k">integer</span>,
                        med_cond5 <span class="pl-k">integer</span>,
                        med_cond6 <span class="pl-k">integer</span>,
                        med_cond7 <span class="pl-k">integer</span>   
                          ) 
    RETURNS <span class="pl-k">numeric</span> <span class="pl-k">AS</span>
    $$
         if (<span class="pl-c1">pg</span>.<span class="pl-c1">state</span>.firstpass <span class="pl-k">==</span> TRUE) {
            <span class="pl-c">#Unserialize the model (i.e reconstruct it from its binary form).</span>
            assign(<span class="pl-s"><span class="pl-pds">"</span>gp_plr_mdl_score<span class="pl-pds">"</span></span>, unserialize(mdl) ,env<span class="pl-k">=</span>.GlobalEnv)
            assign(<span class="pl-s"><span class="pl-pds">"</span>pg.state.firstpass<span class="pl-pds">"</span></span>, FALSE, env<span class="pl-k">=</span>.GlobalEnv)
         }


         <span class="pl-c">#Read the test set from the previously created table  </span>
         test_set <span class="pl-k">&lt;</span><span class="pl-k">-</span> <span class="pl-c1">data</span>.<span class="pl-c1">frame</span>(
                    age <span class="pl-k">=</span> age,
                    gender <span class="pl-k">=</span> gender,
                    race <span class="pl-k">=</span> race,
                    marital_status <span class="pl-k">=</span> marital_status, 
                    bmi <span class="pl-k">=</span>  bmi,
                    med_cond1 <span class="pl-k">=</span>  med_cond1,
                    med_cond2 <span class="pl-k">=</span>  med_cond2,
                    med_cond3 <span class="pl-k">=</span>  med_cond3,
                    med_cond4 <span class="pl-k">=</span>  med_cond4,
                    med_cond5 <span class="pl-k">=</span>  med_cond5,
                    med_cond6 <span class="pl-k">=</span>  med_cond6,
                    med_cond7 <span class="pl-k">=</span>  med_cond7      
                        );
         <span class="pl-c">#Perform prediction</span>
         pred <span class="pl-k">&lt;</span><span class="pl-k">-</span> predict(gp_plr_mdl_score, newdata<span class="pl-k">=</span>test_set, type<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>response<span class="pl-pds">"</span></span>); 

         return (pred)
    $$
    LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;
</pre></div>

<p>You can also score a whole array:</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">DROP</span> <span class="pl-k">FUNCTION</span> IF EXISTS <span class="pl-c1">gpdemo</span>.<span class="pl-c1">mdl_score_demo</span>( <span class="pl-k">bytea</span>, 
                            <span class="pl-k">integer</span>[],
                            <span class="pl-k">text</span>[],
                            <span class="pl-k">text</span>[],
                            <span class="pl-k">text</span>[],
                            <span class="pl-k">double precision</span>[],
                            <span class="pl-k">integer</span>[],
                            <span class="pl-k">integer</span>[],
                            <span class="pl-k">integer</span>[],
                            <span class="pl-k">integer</span>[],
                            <span class="pl-k">integer</span>[],
                            <span class="pl-k">integer</span>[],
                            <span class="pl-k">integer</span>[] 
                              );
    <span class="pl-k">CREATE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">gpdemo</span>.mdl_score_demo( mdl <span class="pl-k">bytea</span>, 
                        age <span class="pl-k">integer</span>[],
                        gender <span class="pl-k">text</span>[],
                        race <span class="pl-k">text</span>[],
                        marital_status <span class="pl-k">text</span>[],
                        bmi <span class="pl-k">double precision</span>[],
                        med_cond1 <span class="pl-k">integer</span>[],
                        med_cond2 <span class="pl-k">integer</span>[],
                        med_cond3 <span class="pl-k">integer</span>[],
                        med_cond4 <span class="pl-k">integer</span>[],
                        med_cond5 <span class="pl-k">integer</span>[],
                        med_cond6 <span class="pl-k">integer</span>[],
                        med_cond7 <span class="pl-k">integer</span>[] 
                          ) 
    RETURNS <span class="pl-k">numeric</span>[]
    IMMUTABLE
    <span class="pl-k">AS</span>
    $$
        gp_plr_mdl_score <span class="pl-k">&lt;</span><span class="pl-k">-</span> unserialize(mdl)

<span class="pl-c">#Read the test set from the previously created table</span>
         test_set <span class="pl-k">&lt;</span><span class="pl-k">-</span> <span class="pl-c1">data</span>.<span class="pl-c1">frame</span>(
                    age <span class="pl-k">=</span> age,
                    gender <span class="pl-k">=</span> gender,
                    race <span class="pl-k">=</span> race,
                    marital_status <span class="pl-k">=</span> marital_status, 
                    bmi <span class="pl-k">=</span>  bmi,
                    med_cond1 <span class="pl-k">=</span>  med_cond1,
                    med_cond2 <span class="pl-k">=</span>  med_cond2,
                    med_cond3 <span class="pl-k">=</span>  med_cond3,
                    med_cond4 <span class="pl-k">=</span>  med_cond4,
                    med_cond5 <span class="pl-k">=</span>  med_cond5,
                    med_cond6 <span class="pl-k">=</span>  med_cond6,
                    med_cond7 <span class="pl-k">=</span>  med_cond7      
                        );
         <span class="pl-c">#Perform prediction</span>
         pred <span class="pl-k">&lt;</span><span class="pl-k">-</span> predict(gp_plr_mdl_score, newdata<span class="pl-k">=</span>test_set, type<span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>response<span class="pl-pds">"</span></span>); 

         return (pred)
    $$
    LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;
</pre></div>

<p>The training, loading and scoring functions can be invoked from SQL like so :</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-c">-- Compute R square (coefficient of determination)</span>
    <span class="pl-c">-- R_square = (1 - SS_err/SS_tot)</span>
    <span class="pl-k">select</span> <span class="pl-s"><span class="pl-pds">'</span>PL/R glm model <span class="pl-pds">'</span></span>::<span class="pl-k">text</span> <span class="pl-k">as</span> model, 
           (<span class="pl-c1">1</span>.<span class="pl-c1">0</span> <span class="pl-k">-</span> <span class="pl-c1">sum</span>(ss_err)<span class="pl-k">*</span><span class="pl-c1">1</span>.<span class="pl-c1">0</span><span class="pl-k">/</span><span class="pl-c1">sum</span>(ss_tot)) <span class="pl-k">as</span> R_square
    <span class="pl-k">from</span>
    (
        <span class="pl-k">select</span> instance_num, 
        (infection_cost_actual <span class="pl-k">-</span> (<span class="pl-k">select</span> <span class="pl-c1">avg</span>(infection_cost) <span class="pl-k">from</span> <span class="pl-c1">gpdemo</span>.<span class="pl-c1">patient_history_test</span>) )^<span class="pl-c1">2</span>.<span class="pl-c1">0</span> <span class="pl-k">as</span> ss_tot,
        (infection_cost_actual <span class="pl-k">-</span>  infection_cost_predicted)^<span class="pl-c1">2</span>.<span class="pl-c1">0</span> <span class="pl-k">as</span> ss_err,      
        <span class="pl-c1">1</span> <span class="pl-k">as</span> cnt
        <span class="pl-k">from</span>
        (
            <span class="pl-c">-- Show actual vs predicted values for the infection cost</span>
            <span class="pl-k">select</span> row_number() over (<span class="pl-k">order by</span> random()) <span class="pl-k">as</span> instance_num, 
                infection_cost <span class="pl-k">as</span> infection_cost_actual,
                <span class="pl-c1">gpdemo</span>.<span class="pl-c1">mdl_score_demo</span> ( <span class="pl-c1">mdls</span>.<span class="pl-c1">model</span>, 
                            age,
                            gender,
                            race,
                            marital_status,
                            bmi,
                            med_cond1,
                            med_cond2,
                            med_cond3,
                            med_cond4,
                            med_cond5,
                            med_cond6,
                            med_cond7       
                              ) <span class="pl-k">as</span> infection_cost_predicted 
            <span class="pl-k">from</span> <span class="pl-c1">gpdemo</span>.<span class="pl-c1">plr_mdls</span> mdls, 
                 <span class="pl-c1">gpdemo</span>.<span class="pl-c1">patient_history_test</span> test 
        ) q1
    ) q2 <span class="pl-k">group by</span> cnt;</pre></div>

<h3>
<a id="-verify-parallelization" class="anchor" href="#-verify-parallelization" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="parallelization"></a> Verify Parallelization</h3>

<p>Congratulations, you've just parallelized your first PL/R algorithm in GPDB/HAWQ!  Or have you? In this section we will describe three sanity checks to ensure that your code is actually running in parallel. </p>

<h4>
<a id="-option-1-via-segment-hostnames" class="anchor" href="#-option-1-via-segment-hostnames" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_parallelization_hostnames"></a> Option 1: Via Segment Hostnames</h4>

<p>We can quickly verify if a PL/R function is indeed running on all segment as follows:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">drop</span> <span class="pl-k">function</span> if exists plr_parallel_test;
<span class="pl-k">create</span> <span class="pl-k">function</span> <span class="pl-en">plr_parallel_test</span>() 
returns <span class="pl-k">text</span> 
<span class="pl-k">as</span> 
$$ 
    return (system(<span class="pl-s"><span class="pl-pds">'</span>hostname<span class="pl-pds">'</span></span>,intern<span class="pl-k">=</span>TRUE)) 
$$ language <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;</pre></div>

<p>The function returns the hostname of the segment node on which it is executing. By invoking the function for rows from a table that is distributed across all segments, we can verify if we indeed
see all the segments in the output.</p>

<div class="highlight highlight-source-sql"><pre>gpadmin<span class="pl-k">=</span><span class="pl-c"># select distinct plr_parallel_test() from abalone;</span>
 plr_parallel_test 
<span class="pl-c">------------------</span>
 sdw1
 sdw10
 sdw11
 sdw12
 sdw13
 sdw14
 sdw15
 sdw16
 sdw2
 sdw3
 sdw4
 sdw5
 sdw6
 sdw7
 sdw8
 sdw9
(<span class="pl-c1">16</span> rows)</pre></div>

<p>We can see that all 16 segment hosts were returned in the result, which means all nodes executed our PL/R function.</p>

<h4>
<a id="-option-2-via-timing" class="anchor" href="#-option-2-via-timing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_parallelization_timing"></a> Option 2: Via Timing</h4>

<p>An alternative way to verify whether your code is running in parallel is to do timed performance testing. This method is laborious, but can be helpful in precisely communicating the speedup achieved through parallelization to a business partner or customer. Using the abalone dataset, we show how to compare the timing results from an implementation that builds models sequentially with a version that builds models in parallel. </p>

<p>First we create a PL/R function which builds a linear regression to predict the age of an abalone (determined by counting the number of rings) from physical measurements. The function returns the coefficients for each of the linear predictors. </p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">DROP</span> <span class="pl-k">FUNCTION</span> IF EXISTS plr_lm( sex <span class="pl-k">text</span>[], length float8[], diameter float8[],
            height float8[], whole_weight float8[], 
            shucked_weight float8[], viscera_weight float8[], 
            shell_weight float8[], rings float8[] );
    <span class="pl-k">CREATE OR REPLACE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">plr_lm</span>( sex <span class="pl-k">text</span>[], length float8[], 
            diameter float8[], height float8[], whole_weight float8[], 
            shucked_weight float8[], viscera_weight float8[], 
            shell_weight float8[], rings float8[] ) 
    RETURNS FLOAT8[] <span class="pl-k">AS</span> 
    $$
      abalone   <span class="pl-k">=</span> <span class="pl-c1">data</span>.<span class="pl-c1">frame</span>( sex, length, diameter, height, whole_weight, 
            shucked_weight, viscera_weight, shell_weight, rings ) 

      m <span class="pl-k">=</span> lm(formula <span class="pl-k">=</span> rings ~ ., data <span class="pl-k">=</span> abalone)

      coef( m )
    $$
    LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;</pre></div>

<p>Next we convert the dataset to an array representation (as described in <a href="#dataprep">Data Preparation</a>) and store the results in a new table called <code>abalone_array</code>.</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-c">-- Create a vectorized version of the data</span>
    <span class="pl-c">-- This table has a single row, and 9 columns</span>
    <span class="pl-c">-- Each element contains all of the elements for the</span>
    <span class="pl-c">-- respective column as an array </span>
    <span class="pl-k">DROP</span> <span class="pl-k">TABLE</span> IF EXISTS abalone_array;
    <span class="pl-k">CREATE</span> <span class="pl-k">TABLE</span> <span class="pl-en">abalone_array</span> <span class="pl-k">AS</span> 
    <span class="pl-k">SELECT</span> 
      array_agg(sex)::<span class="pl-k">text</span>[] <span class="pl-k">as</span> sex, 
      array_agg(length)::float8[] <span class="pl-k">as</span> length,
      array_agg(diameter)::float8[] <span class="pl-k">as</span> diameter, 
      array_agg(height)::float8[] <span class="pl-k">as</span> height,
      array_agg(whole_weight)::float8[] <span class="pl-k">as</span> whole_weight, 
      array_agg(shucked_weight)::float8[] <span class="pl-k">as</span> shucked_weight,
      array_agg(viscera_weight)::float8[] <span class="pl-k">as</span> viscera_weight, 
      array_agg(shell_weight)::float8[] <span class="pl-k">as</span> shell_weight, 
      array_agg(rings)::float8[] <span class="pl-k">as</span> rings
    <span class="pl-k">FROM</span> abalone
    DISTRIBUTED RANDOMLY;</pre></div>

<p>Now that we have a PL/R function definition and the dataset prepared in an array representation, we can call the function like this:</p>

<pre><code>    SELECT plr_lm( sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings )
    FROM abalone_array;
    ---------------
    (1 row)

    Time: 47.341 ms
</code></pre>

<p>Note that creating a single model takes about 47 ms. </p>

<p>But what if we want to create multiple models? For instance, imagine the abalone were sampled from 64 different regions and we hypothesize that the physical characteristics vary based on region. In this situation, we may want to construct multiple models to capture the region-specific effects. To simulate this scenario we will simply replicate the same dataset 64 times and build 64 identical models. We construct the models sequentially and in parallel and compare the execution time. </p>

<p>To build the models sequentially we create a simple PGSQL function that builds linear models in a loop using the <code>plr_lm</code> function we created earlier: </p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">DROP</span> <span class="pl-k">FUNCTION</span> IF EXISTS IterativePLRModels( <span class="pl-k">INTEGER</span> );
    <span class="pl-k">CREATE OR REPLACE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">IterativePLRModels</span>( <span class="pl-k">INTEGER</span> ) 
    RETURNS SETOF <span class="pl-k">TEXT</span> 
    <span class="pl-k">AS</span> $BODY$
    DECLARE
      n ALIAS FOR $<span class="pl-c1">1</span>;
    <span class="pl-k">BEGIN</span>
      FOR i <span class="pl-k">IN</span> <span class="pl-c1">1</span>..n LOOP
        RAISE NOTICE <span class="pl-s"><span class="pl-pds">'</span>Processing %<span class="pl-pds">'</span></span>, i;
        PERFORM plr_lm( sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings )
        <span class="pl-k">FROM</span> abalone_array;
        RETURN NEXT i::<span class="pl-k">TEXT</span>;
      END LOOP;
    END
    $BODY$
      LANGUAGE plpgsql;</pre></div>

<p>The function accepts a single argument, which specifies the number of iterations. For this example we set that value to 64 and expect that the running time will be roughly the length of time it took to build a single model multiplied by the number of iterations: 47 * 64 = 3008 ms.</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">SELECT</span> IterativePLRModels( <span class="pl-c1">64</span> );
    <span class="pl-c">-----------</span>
    (<span class="pl-c1">64</span> rows)

    <span class="pl-k">Time</span>: <span class="pl-c1">2875</span>.<span class="pl-c1">609</span> ms</pre></div>

<p>Pretty darn close!</p>

<p>Next let's construct the models in parallel. In order to do this we must replicate the abalone data and distribute it across the GPDB segments. The PGSQL function below creates a new table called <code>abalone_array_replicates</code> that contains copies of the abalone dataset indexed by a <code>distkey</code> and distributed randomly across the segments. </p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">DROP</span> <span class="pl-k">FUNCTION</span> IF EXISTS ReplicateAbaloneArrays( <span class="pl-k">INTEGER</span> );
    <span class="pl-k">CREATE OR REPLACE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">ReplicateAbaloneArrays</span>( <span class="pl-k">INTEGER</span> ) 
    RETURNS <span class="pl-k">INTEGER</span> <span class="pl-k">AS</span>
    $BODY$
    DECLARE
      n ALIAS FOR $<span class="pl-c1">1</span>;
    <span class="pl-k">BEGIN</span>
      <span class="pl-k">DROP</span> <span class="pl-k">TABLE</span> IF EXISTS abalone_array_replicates;
      <span class="pl-k">CREATE</span> <span class="pl-k">TABLE</span> <span class="pl-en">abalone_array_replicates</span> <span class="pl-k">AS</span>
      <span class="pl-k">SELECT</span> <span class="pl-c1">1</span> <span class="pl-k">as</span> distkey, <span class="pl-k">*</span> <span class="pl-k">FROM</span> abalone_array
      DISTRIBUTED randomly;

      FOR i <span class="pl-k">IN</span> <span class="pl-c1">2</span>..n LOOP
        <span class="pl-k">INSERT INTO</span> abalone_array_replicates <span class="pl-k">SELECT</span> i <span class="pl-k">as</span> distkey, <span class="pl-k">*</span> <span class="pl-k">FROM</span> abalone_array;
      END LOOP;

      RETURN n;
    END;
    $BODY$
      LANGUAGE plpgsql;</pre></div>

<p>The function accepts a single argument, which specifies the number of copies to make: </p>

<pre><code>    -- Create 64 copies
    SELECT ReplicateAbaloneArrays( 64 );
</code></pre>

<p>Now we have a new table <code>abalone_array_replicates</code> that contains 64 rows and 9 columns in array representation, simulating measurements of 64 different types of abalone collected from different regions. We are now ready to construct 64 models in parallel. If the parallelization were perfectly efficient, the expected running time would be the running time of a single model, multiplied by the number of models, divided by the number of segments: (47 * 64) / 96 ~= 31 ms!</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">SELECT</span> plr_lm( sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings )
    <span class="pl-k">FROM</span> abalone_array_replicates;
    <span class="pl-c">-----------------</span>
    (<span class="pl-c1">64</span> rows)

    <span class="pl-k">Time</span>: <span class="pl-c1">183</span>.<span class="pl-c1">937</span> ms</pre></div>

<p>Of course, parallelization aint perfect! There is overhead associated with parallel processing. However, the contribution of the overhead to the overall running time of an algorithm shrinks as the size of the data increase. Additionally, since the distribution function is <code>random</code> data are not necessarily <em>uniformly</em> distributed across segments. You can see how the data are distributed by interrogating the database like this:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span> gp_segment_id, <span class="pl-c1">count</span>(<span class="pl-k">*</span>)
<span class="pl-k">FROM</span> abalone_array_replicates
<span class="pl-k">GROUP BY</span> gp_segment_id
<span class="pl-k">ORDER BY</span> gp_segment_id;</pre></div>

<p>If you plot the results in R:</p>

<div class="highlight highlight-source-r"><pre>barplot( <span class="pl-smi">segment_distribution</span>, <span class="pl-v">xlab</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>Segment ID<span class="pl-pds">'</span></span>, <span class="pl-v">ylab</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>Number of rows<span class="pl-pds">'</span></span>, <span class="pl-v">main</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>Row distribution w/ sequential dist key<span class="pl-pds">'</span></span> )</pre></div>

<p>You will get a plot that looks something like the one below. Note that certain segments (64, 61) have 3 models to build, while others only have 1. The overall running time of the algorithm is bounded by the running time of the slowest node - a good reminder of why it is important to choose your distribution key wisely!</p>

<p><img src="https://github.com/zimmeee/gp-r/blob/master/figures/RowDistAcrossSegments.png?raw=true" alt="alt text" title="Row distribution across segments"></p>

<h4>
<a id="-option-3-via-pivotal-command-center" class="anchor" href="#-option-3-via-pivotal-command-center" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_parallelization_cc"></a> Option 3: Via Pivotal Command Center</h4>

<p>A heuristic, visual option to verify parallelism is via the Pivotal Command Center.  You would want to start by logging into Pivotal Command Center, and navigating to the 'Realtime (By Server)' menu under the 'System Metrics' tab.  Below is an example of how this page should look if your database is idle:</p>

<p><img src="https://github.com/wjjung317/gp-r/blob/master/figures/commandcenter_idle.png?raw=true" alt="alt text" title="Snapshot of Pivotal Command Center When DB is Idle"></p>

<p>Suppose that you have now successfully implemented a parallelized PL/R function.  While the function is executing, check back on that same page on Pivotal Command Center - it should look like the following.  Note that the CPU panel shows activity for multiple database segments - if the function was not successfully parallelized, then only a single segment would show CPU activity.</p>

<p><img src="https://github.com/wjjung317/gp-r/blob/master/figures/commandcenter_parallelized.png?raw=true" alt="alt text" title="Snapshot of Pivotal Command Center When DB is Executing a Parallelized PL/R Function"></p>

<h2>
<a id="-more-details" class="anchor" href="#-more-details" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_details"></a> More Details</h2>

<h3>
<a id="-data-types" class="anchor" href="#-data-types" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="datatypes"></a> Data Types</h3>

<p>At its core, a function takes in input, does something with this input, and produces output.  PL/R functions in GPDB/HAWQ:</p>

<ol>
<li> Take SQL data types as input</li>
<li> Converts SQL data types to R data types</li>
<li> Outputs results as R data types</li>
<li> Converts the R data type output as SQL data types</li>
</ol>

<p>(1) and (3) are fairly straightforward.  We personally found (2) and (4) a little less straightforward, and would like to devote some space to go into these two pieces in more detail.  </p>

<p>The purpose of this section is really to just help users be aware of default data type conversions, and keep them in mind when doing code development and debugging.</p>

<p>It is our subjective view that being familiar with the treatment of multi-element data types is generally more useful for day-to-day data science.  We focus on PL/R’s default treatment of multi-element numeric data types rather than scalars or text values.  Material on scalars and text will soon follow.  </p>

<h4>
<a id="-plr-input-conversion-sql-data-types--r-data-types" class="anchor" href="#-plr-input-conversion-sql-data-types--r-data-types" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_datatypes_input"></a> PL/R Input Conversion: SQL Data Types → R Data Types</h4>

<p>We will describe how SQL data types are converted into R data types via PL/R in this section.  </p>

<p>Let’s take a look at some examples.  We first define a PL/R function that simply returns a string of identifying the R data type:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">DROP</span> <span class="pl-k">FUNCTION</span> IF EXISTS func_array(arg_array float8[]);
<span class="pl-k">CREATE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">func_array</span>(arg_array float8[]) 
RETURNS <span class="pl-k">text</span> <span class="pl-k">AS</span> 
$$ 
d<span class="pl-k">&lt;</span><span class="pl-k">-</span> arg_array
return(class(d))
$$
LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;</pre></div>

<p>You would think that 1D SQL arrays (i.e. a vector of values) should map to R vectors, but we see that 1D SQL arrays default-map to 1D R arrays:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span> array[<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>,<span class="pl-c1">4</span>];
   array   
<span class="pl-c">-----------</span>
 {<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>,<span class="pl-c1">4</span>}
(<span class="pl-c1">1</span> row)

<span class="pl-k">SELECT</span> func_array(array[<span class="pl-c1">1</span>,<span class="pl-c1">2</span>,<span class="pl-c1">3</span>,<span class="pl-c1">4</span>]);
func_array 
<span class="pl-c">------------</span>
 array
(<span class="pl-c1">1</span> row)
</pre></div>

<p>Given the result for 1D SQL arrays, what are your bets on how 2D SQL arrays are mapped to R objects?  Turns out that 2D SQL arrays (i.e. a matrix) default-map to R matrices (not R 2D arrays):</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span> array[array[<span class="pl-c1">1</span>,<span class="pl-c1">2</span>], array[<span class="pl-c1">3</span>,<span class="pl-c1">4</span>]];
     array     
<span class="pl-c">---------------</span>
 {{<span class="pl-c1">1</span>,<span class="pl-c1">2</span>},{<span class="pl-c1">3</span>,<span class="pl-c1">4</span>}}
(<span class="pl-c1">1</span> row)
<span class="pl-k">SELECT</span> func_array(array[array[<span class="pl-c1">1</span>,<span class="pl-c1">2</span>],array[<span class="pl-c1">3</span>,<span class="pl-c1">4</span>]]);
 func_array 
<span class="pl-c">------------</span>
 matrix
(<span class="pl-c1">1</span> row)</pre></div>

<p>And as one would expect, 3D SQL arrays map to an R array:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span> array[array[array[<span class="pl-c1">1</span>,<span class="pl-c1">2</span>], array[<span class="pl-c1">3</span>,<span class="pl-c1">4</span>]],array[array[<span class="pl-c1">5</span>,<span class="pl-c1">6</span>], array[<span class="pl-c1">7</span>,<span class="pl-c1">8</span>]]];
             array             
<span class="pl-c">-------------------------------</span>
 {{{<span class="pl-c1">1</span>,<span class="pl-c1">2</span>},{<span class="pl-c1">3</span>,<span class="pl-c1">4</span>}},{{<span class="pl-c1">5</span>,<span class="pl-c1">6</span>},{<span class="pl-c1">7</span>,<span class="pl-c1">8</span>}}}
(<span class="pl-c1">1</span> row)
<span class="pl-k">SELECT</span> func_array(array[array[array[<span class="pl-c1">1</span>,<span class="pl-c1">2</span>], array[<span class="pl-c1">3</span>,<span class="pl-c1">4</span>]],array[array[<span class="pl-c1">5</span>,<span class="pl-c1">6</span>], array[<span class="pl-c1">7</span>,<span class="pl-c1">8</span>]]]);
func_array 
<span class="pl-c">------------</span>
 array
(<span class="pl-c1">1</span> row)</pre></div>

<p>You can of course convert between data types in R, so if an R function that you’d like to use in your workflow expects data to be in a certain R class, just make appropriate conversions in your PL/R code:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">DROP</span> <span class="pl-k">FUNCTION</span> IF EXISTS func_convert_example(arg_array float8[]);
<span class="pl-k">CREATE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">func_convert_example</span>(arg_array float8[]) 
RETURNS <span class="pl-k">text</span> <span class="pl-k">AS</span> 
$$ 
d<span class="pl-k">&lt;</span><span class="pl-k">-</span> arg_array
d<span class="pl-k">&lt;</span><span class="pl-k">-</span> <span class="pl-k">as</span>.<span class="pl-c1">data</span>.<span class="pl-c1">frame</span>(d)
return(class(d))
$$
LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;

<span class="pl-k">SELECT</span> func_convert_example(array[array[<span class="pl-c1">1</span>,<span class="pl-c1">2</span>], array[<span class="pl-c1">3</span>,<span class="pl-c1">4</span>]]); 
func_convert_example 
<span class="pl-c">----------------------</span>
 <span class="pl-c1">data</span>.<span class="pl-c1">frame</span>
(<span class="pl-c1">1</span> row)</pre></div>

<h4>
<a id="-plr-output-conversion-r-data-types--sql-data-types" class="anchor" href="#-plr-output-conversion-r-data-types--sql-data-types" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plr_datatypes_output"></a> PL/R Output Conversion: R Data Types → SQL Data Types</h4>

<p>For multi-element returns from a PL/R function, you generally have two options.  Multi-element return objects from PL/R can be expressed as:</p>

<ol>
<li> A SQL array (in all flavors: 1D,2D,3D), or </li>
<li> A SQL composite type</li>
</ol>

<p>The quickest, “hands-free” approach is to just specify your return object as a SQL array.  Regardless of whether your R object is a vector, matrix, data.frame, or array, you will be able to recover the information contained in the R object by specifying a SQL array as your RETURN data type for a given PL/R function.</p>

<ul>
<li>Vectors, a single column of a matrix or data.frame, and a 1D R array are returned as a 1D SQL array</li>
<li>A matrix, a data.frame, and a 2D R array are returned as a 2D SQL array</li>
<li>A 3D R array is returned as a 3D SQL array</li>
</ul>

<p>A couple of caveats here.  Arrays can be somewhat difficult to look at in SQL.  Also, there currently isn’t support for arrays of mixed type.  You can nominally set your return type to a text[], but this will find limited use in an analytics workflow.</p>

<p>A richer, more flexible approach is to use a SQL composite type as your RETURN data type for a given PL/R function.  Let’s suppose you wanted to return the equivalent of an R data frame in your PL/R function.  In other words, lets suppose you’d like to return a table where at least one of the columns contains text rather than numbers.  We allow for this return by first setting up a SQL composite type in Greenplum.  You can think of SQL composite types as a “template” or “skeleton” for SQL tables.  When setting up a type, it’s useful to think ahead and draw out the format of the output you intend to get back from your PL/R function.</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">DROP</span> <span class="pl-k">TYPE</span> IF EXISTS iris_type CASCADE;
<span class="pl-k">CREATE</span> <span class="pl-k">TYPE</span> <span class="pl-en">iris_type</span> <span class="pl-k">AS</span> (
sepal_length float8, sepal_width float8, petal_length float8, petal_width float8, specices <span class="pl-k">text</span>);</pre></div>

<p>We can then return output from a PL/R function which follows the structure of the type you’ve created.  You just need to specify your return type as a SETOF your custom type:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">CREATE OR REPLACE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">iris_trivial</span> ()  
RETURNS SETOF iris_type <span class="pl-k">AS</span> 
$$ 
data(iris)
d<span class="pl-k">&lt;</span><span class="pl-k">-</span> iris
return(d[c(<span class="pl-c1">1</span>,<span class="pl-c1">51</span>,<span class="pl-c1">100</span>),])
$$
LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;

<span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">from</span> iris_trivial();
sepal_length | sepal_width | petal_length | petal_width |  specices  
<span class="pl-c">--------------+-------------+--------------+-------------+------------</span>
          <span class="pl-c1">5</span>.<span class="pl-c1">1</span> |         <span class="pl-c1">3</span>.<span class="pl-c1">5</span> |          <span class="pl-c1">1</span>.<span class="pl-c1">4</span> |         <span class="pl-c1">0</span>.<span class="pl-c1">2</span> | setosa
            <span class="pl-c1">7</span> |         <span class="pl-c1">3</span>.<span class="pl-c1">2</span> |          <span class="pl-c1">4</span>.<span class="pl-c1">7</span> |         <span class="pl-c1">1</span>.<span class="pl-c1">4</span> | versicolor
          <span class="pl-c1">5</span>.<span class="pl-c1">7</span> |         <span class="pl-c1">2</span>.<span class="pl-c1">8</span> |          <span class="pl-c1">4</span>.<span class="pl-c1">1</span> |         <span class="pl-c1">1</span>.<span class="pl-c1">3</span> | versicolor
(<span class="pl-c1">3</span> rows)</pre></div>

<p>The data types for the individual columns are governed by those of the SQL composite defined:</p>

<div class="highlight highlight-source-sql"><pre><span class="pl-k">DROP</span> <span class="pl-k">TABLE</span> IF EXISTS iris_trivial_table;
<span class="pl-k">CREATE</span> <span class="pl-k">TABLE</span> <span class="pl-en">iris_trivial_table</span> <span class="pl-k">AS</span> <span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> iris_trivial();
\d<span class="pl-k">+</span> iris_trivial_table
                  Table <span class="pl-s"><span class="pl-pds">"</span>public.iris_trivial_table<span class="pl-pds">"</span></span>
    Column    |       Type       | Modifiers | Storage  | Description 
<span class="pl-c">--------------+------------------+-----------+----------+-------------</span>
 sepal_length | <span class="pl-k">double precision</span> |           | plain    | 
 sepal_width  | <span class="pl-k">double precision</span> |           | plain    | 
 petal_length | <span class="pl-k">double precision</span> |           | plain    | 
 petal_width  | <span class="pl-k">double precision</span> |           | plain    | 
 specices     | <span class="pl-k">text</span>             |           | extended | </pre></div>

<p>We see that this is identical to the set of column data types of iris_type.</p>

<h3>
<a id="-memory-limits" class="anchor" href="#-memory-limits" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="memory"></a> Memory Limits</h3>

<p>When coding in PL/R there are a couple of memory management items to keep in mind.  </p>

<p>Recall that R is installed on each and every host of the Greenplum database - one corrollary is that each "mapper" job which you wish to execute in parallel via PL/R must fit in the memory of the R on each host.  </p>

<p>Given the heavy use of arrays in a PL/R workflow, another item to keep in mind is that the maximum memory limit for each cell (i.e. each record-column tuple) in GPDB/HAWQ database is 1GB.  This is a theoretical upper bound and in practice, the maximum can be less than 1GB.  </p>

<h1>
<a id="plr-exercises" class="anchor" href="#plr-exercises" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plrexercises"></a>PL/R Exercises</h1>

<p>The folder <a href="https://github.com/pivotalsoftware/gp-r/tree/master/exercises">PL/R Exercises</a> contains 3 different exercises we developed for the <a href="http://pivotal.io/training">Data Science in Practice</a> course using publicly available datasets. The first demonstrates
using Ridge Regression from the <code>MASS</code> package, the second demonstrates using decision trees in the <code>rpart</code> package while the third is an exercises onRandom Forests using the <code>randomForest</code> package. The solutions are included inline in the same file. </p>

<h1>
<a id="-rpostgresql-on-greenplum--hawq" class="anchor" href="#-rpostgresql-on-greenplum--hawq" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="rpostgresql"></a> RPostgreSQL on Greenplum &amp; HAWQ</h1>

<h2>
<a id="overview" class="anchor" href="#overview" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

<p>The <a href="http://cran.r-project.org/web/packages/RPostgreSQL/index.html">RPostgreSQL package</a> provides a database interface and PostgreSQL driver for R that is compatible with GDPB/HAWQ. This connection can be used to query GDPB/HAWQ in the normal fashion from within R code. We have found this package to be helpful for prototyping, working with datasets that can fit in-memory, and building visualizations. Generally speaking, using the RPostgreSQL interface does not lend itself to parallelization.  </p>

<p>Using RPostgreSQL with a database includes the following 3 steps: </p>

<ol>
<li>     Create a database driver for PostgreSQL, </li>
<li>     Connect to a specific database, and </li>
<li>     Execute the query on GPDB/HAWQ and return results </li>
</ol>

<h2>
<a id="-local-development" class="anchor" href="#-local-development" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="rpostgresql_local"></a> Local Development</h2>

<p>RPostgreSQL can be used in a local development environment to connect to a remote GPDB/HAWQ instance. Queries are processed in parallel on GPDB/HAWQ and results are returned in the familiar R data frame format. Use caution when returning large resultsets as you may run into the memory limitations of your local R instance. To ease troubleshooting, it can be helpful to develop/debug the SQL using your GPDB tool of choice (e.g. pgAdmin) before using it in R. </p>

<div class="highlight highlight-source-r"><pre>    <span class="pl-v">DBNAME</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>marketing<span class="pl-pds">'</span></span>
    <span class="pl-v">HOST</span>   <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>10.110.134.123<span class="pl-pds">'</span></span>

    <span class="pl-c"># Create a driver</span>
    <span class="pl-smi">drv</span> <span class="pl-k">&lt;-</span> dbDriver( <span class="pl-s"><span class="pl-pds">"</span>PostgreSQL<span class="pl-pds">"</span></span> )
    <span class="pl-c"># Create the database connection</span>
    <span class="pl-smi">con</span> <span class="pl-k">&lt;-</span> dbConnect( <span class="pl-smi">drv</span>, <span class="pl-v">dbname</span> <span class="pl-k">=</span> <span class="pl-smi">DBNAME</span>, <span class="pl-v">host</span> <span class="pl-k">=</span> <span class="pl-smi">HOST</span> )

    <span class="pl-c"># Create the SQL query string. Include a semi-colon to terminate</span>
    <span class="pl-v">querystring</span> <span class="pl-k">=</span>   <span class="pl-s"><span class="pl-pds">'</span>SELECT countryname, income, babies FROM country_table;<span class="pl-pds">'</span></span>
    <span class="pl-c"># Execute the query and return results as a data frame</span>
    <span class="pl-v">countries</span>   <span class="pl-k">=</span> dbGetQuery( <span class="pl-smi">con</span>, <span class="pl-smi">querystring</span> )

    <span class="pl-c"># Plot the results</span>
    plot( <span class="pl-smi">countries</span><span class="pl-k">$</span><span class="pl-smi">income</span>, <span class="pl-smi">countries</span><span class="pl-k">$</span><span class="pl-smi">babies</span> )</pre></div>

<h2>
<a id="-plotting" class="anchor" href="#-plotting" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="plotting"></a> Plotting</h2>

<p>It is probably best to do plotting on a single node (either the master or locally using the RPostgreSQL interface). In this context, plotting is no different from normal plotting in R. Of course, you likely have <em>a lot</em> of data which may obscure traditional visualization techniques. You may choose to experiment with packages like <a href="https://github.com/hadley/bigvis">bigviz</a> which provides tools for exploratory data analysis of large datasets. </p>

<h2>
<a id="-caveats-around-usage-within-plr" class="anchor" href="#-caveats-around-usage-within-plr" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="rpostgresql_plrcaveats"></a> Caveats Around Usage Within PL/R</h2>

<p>RPostgreSQL can also be used from within a PL/R function and deployed on the host GPDB instance. This bypasses the PL/R pipe for data exchange in favor of the DBI driver used by RPostgreSQL. The primary benefit of using this interface over the standard PL/R interface is that datatype conversions happen automatically; one need not specify all of the columns and their datatypes to pass to the function ahead of time. Sensible conversions are done automatically, including conversion of strings to factors which can be helpful in downstream processes. </p>

<p>While RPostgreSQL can be quite useful in a development context, don't be fooled. It is not a good path towards actual parallelization of your R code. Because the code in the PL/R function accesses database objects it cannot safely be called in a distributed manner. This will lead to errors such as:</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">DROP</span> <span class="pl-k">FUNCTION</span> IF EXISTS my_plr_error_func( character );
    <span class="pl-k">CREATE OR REPLACE</span> <span class="pl-k">FUNCTION</span> <span class="pl-en">my_plr_error_func</span>( character ) 
    RETURNS <span class="pl-k">INTEGER</span> <span class="pl-k">AS</span> 
    $$
      library(<span class="pl-s"><span class="pl-pds">"</span>RPostgreSQL<span class="pl-pds">"</span></span>)

      drv <span class="pl-k">&lt;</span><span class="pl-k">-</span> dbDriver( <span class="pl-s"><span class="pl-pds">"</span>PostgreSQL<span class="pl-pds">"</span></span> )
      con <span class="pl-k">&lt;</span><span class="pl-k">-</span> dbConnect( drv, dbname <span class="pl-k">=</span> arg1 )

      querystring <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">'</span>SELECT reviewid FROM sample_model_data;<span class="pl-pds">'</span></span>
      <span class="pl-c1">model</span>.<span class="pl-c1">data</span>  <span class="pl-k">=</span> dbGetQuery( con, querystring )

      <span class="pl-c1">16</span>
    $$
    LANGUAGE <span class="pl-s"><span class="pl-pds">'</span>plr<span class="pl-pds">'</span></span>;</pre></div>

<p>This returns without error, but does not run in parallel</p>

<div class="highlight highlight-source-sql"><pre>    <span class="pl-k">SELECT</span> my_plr_error_func( <span class="pl-s"><span class="pl-pds">'</span>zimmen<span class="pl-pds">'</span></span> );</pre></div>

<p>This produces the error below</p>

<pre><code>    SELECT my_plr_error_func( 'zimmen' ) 
    FROM sample_model_data;

    ********** Error **********

    ERROR: R interpreter expression evaluation error  (seg55 slice1 sdw3:40001 pid=1676)
    SQL state: 22000
    Detail: 
         Error in pg.spi.exec(sql) : 
      error in SQL statement : function cannot execute on segment because it accesses relation "public.sample_model_data"
         In R support function pg.spi.exec
    In PL/R function my_plr_error_func
</code></pre>

<p>GPDB is complaining because you are trying to access a table directly from a segment, which breaks the whole notion of coordination between the master node and its segments. Therefore, you cannot specify a <code>FROM</code> clause in your PL/R function when you make an RPostgreSQL call from within that function. </p>

<h4>
<a id="alternative" class="anchor" href="#alternative" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Alternative</h4>

<p>For the adventerous, the RPostgreSQL package provides more granular control over execution. An equivalent to dbGetQuery is to first submit the SQL to the database engine using dbSendQuery and then fetch the results: </p>

<div class="highlight highlight-source-r"><pre><span class="pl-smi">drv</span> <span class="pl-k">&lt;-</span> dbDriver( <span class="pl-s"><span class="pl-pds">"</span>PostgreSQL<span class="pl-pds">"</span></span> )
<span class="pl-smi">con</span> <span class="pl-k">&lt;-</span> dbConnect( <span class="pl-smi">drv</span> )
<span class="pl-smi">res</span> <span class="pl-k">&lt;-</span> dbSendQuery( <span class="pl-smi">con</span>, <span class="pl-s"><span class="pl-pds">"</span>SELECT * FROM sample_model_data;<span class="pl-pds">"</span></span> )
<span class="pl-smi">data</span> <span class="pl-k">&lt;-</span> fetch( <span class="pl-smi">res</span>, <span class="pl-v">n</span> <span class="pl-k">=</span> <span class="pl-k">-</span><span class="pl-c1">1</span> ) </pre></div>

<p>Note that the fetch function has a parameter, <code>n</code>, which sets the maximum number of records to retrieve. You probably always want to set this value to -1 to retrieve all of the records. I'm not sure why you would ever use this instead of the simpler dbGetQuery. </p>

<h1>
<a id="-pivotalr-on-greenplum--hawq" class="anchor" href="#-pivotalr-on-greenplum--hawq" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="pivotalr"></a> PivotalR on Greenplum &amp; HAWQ</h1>

<h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

<p><a href="http://madlib.incubator.apache.org">Apache MADlib</a> is an open-source library for highly scalable in-database/in-Hadoop analytics, and it currently runs on GPDB, HAWQ, and PostgreSQL.  MADlib provides implicitly parallelized SQL implementations of statistical &amp; machine learning models that run directly inside of GPDB, HAWQ, and PostgreSQL. Examples of algorithms currently available in MADlib include linear regression, logistic regression, multinomial regression, elastic net, ARIMA, k-means clustering, naïve bayes, decision trees, random forests, support vector machines, Cox proportional hazards, time series analysis, conditional random fields, association rules, and latent dirichlet allocation.  </p>

<p>While end users benefit from MADlib’s high performance and scalability, its audience has previously been focused to those who are comfortable with modeling in SQL. <a href="http://cran.r-project.org/web/packages/PivotalR/">PivotalR</a> is an R package that allows practitioners who know R but very little SQL to leverage the performance and scalability benefits of in-database/in-Hadoop processing.  </p>

<p>The debut release of PivotalR was shipped out in June 2013.  A quickstart guide to PivotalR is available <a href="https://github.com/wjjung317/gp-r/blob/master/docs/PivotalR-quick-start%20v2.pdf">here</a>.  There is active ongoing development of  PivotalR, and we encourage you to view or contribute to this work on its <a href="https://github.com/pivotalsoftware/PivotalR">GitHub Page</a>.</p>

<h2>
<a id="-design--features" class="anchor" href="#-design--features" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="pivotalr_design"></a> Design &amp; Features</h2>

<p><img src="https://github.com/wjjung317/gp-r/blob/master/figures/PivotalR.png?raw=true" alt="alt text" title="PivotalR Design"></p>

<p>At its core, an R function in PivotalR:</p>

<ol>
<li>Translates R model formulas into corresponding SQL statements</li>
<li>Executes these statements on the cluster</li>
<li>Returns summarized model output to R </li>
</ol>

<p>This allows R users to leverage the scalability and performance of in-database/in-Hadoop analytics without leaving the R command line. All of the computational heavy lifting is executed in-database, while the end user benefits from a familiar R interface.  Compared with respective native R functions, we observe a dramatic increase in scalability and a decrease in running time, even after normalizing for hardware differences. Furthermore, data movement -- which can take hours/days for big data -- is eliminated via PivotalR.  </p>

<p>Key features include the following:</p>

<ul>
<li>All data stays in DB: R objects merely point to DB objects</li>
<li>All model estimation and heavy lifting done in DB via MADlib </li>
<li>R → SQL translation done via PivotalR</li>
<li>Only strings of SQL and model output transferred across RPostgreSQL -- trivial data transfer</li>
</ul>

<h2>
<a id="-demo" class="anchor" href="#-demo" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="pivotalr_demo"></a> Demo</h2>

<p>We have put together a <a href="http://www.youtube.com/watch?v=6cmyRCMY6j0">video demo</a> of the debut release of PivotalR.  We also provide the <a href="https://github.com/wjjung317/gp-r/blob/master/docs/PivotalR_Demo.pptx">deck</a>, <a href="https://github.com/wjjung317/gp-r/blob/master/src/R/PivotalR_Demo.R">code</a>, and <a href="https://drive.google.com/file/d/0B76GEdSVCa8NUlZhQnFBaGgyTk0/view?usp=sharing">data</a> used in the demo. Note that the demo intends to highlight a selection of functionality in PivotalR - we encourage you to check out the <a href="http://cran.r-project.org/web/packages/PivotalR/PivotalR.pdf">documentation</a> and this <a href="https://journal.r-project.org/archive/2014-1/qian.pdf">paper</a> published in the R Journal to explore more of its features.  </p>

<h2>
<a id="-download--installation" class="anchor" href="#-download--installation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="pivotalr_install"></a> Download &amp; Installation</h2>

<p>PivotalR is available for download and installation from <a href="http://cran.r-project.org/web/packages/PivotalR/">CRAN</a> and its <a href="https://github.com/gopivotal/PivotalR">GitHub Page</a>.</p>

<h1>
<a id="-work-in-progress-shiny-apps-on-cloud-foundry" class="anchor" href="#-work-in-progress-shiny-apps-on-cloud-foundry" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a name="shiny_cf"></a> [WORK IN PROGRESS] Shiny Apps on Cloud Foundry</h1>

<h2>
<a id="overview-1" class="anchor" href="#overview-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Overview</h2>

<p>In this guide, we will assume that the reader is familiar with the <a href="http://shiny.rstudio.com/">Shiny</a> framework for building apps and dashboards.  Background on Shiny and those who need a refresher are encouraged to look <a href="http://shiny.rstudio.com/tutorial/">here</a>.  </p>

<p>We place our focus on helping you get started on hosting Shiny apps on Cloud Foundry.  Please keep in mind that the authors of this current page are data scientists, not application developers.  The instructions here are intended merely to help get you started -- readers are encouraged to consult other resources (i.e. <a href="http://12factor.net/">here</a>) and ideally your <a href="http://pivotallabs.com">developer &amp; designer</a> buddies to improve and optimize.</p>

<h2>
<a id="bare-mininum-requirements-for-hosting-shiny-apps-on-cf" class="anchor" href="#bare-mininum-requirements-for-hosting-shiny-apps-on-cf" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Bare Mininum Requirements for Hosting Shiny Apps on CF</h2>

<ul>
<li>CF environment</li>
<li>R buildpack</li>
<li>Shiny Code</li>
<li>init.r file</li>
<li>startscript.r file</li>
<li>manifest.yml file </li>
</ul>

<h3>
<a id="cf-environment" class="anchor" href="#cf-environment" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>CF environment</h3>

<ul>
<li>Set API endpoint </li>
<li>Login with your username/password</li>
</ul>

<h3>
<a id="r-buildpack" class="anchor" href="#r-buildpack" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>R buildpack</h3>

<ul>
<li>
<a href="https://github.com/wjjung317/heroku-buildpack-r">https://github.com/wjjung317/heroku-buildpack-r</a>
[more details to be filled out]</li>
</ul>

<h3>
<a id="shiny-code" class="anchor" href="#shiny-code" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Shiny Code</h3>

<p>Create a folder within your app directory and store the following two files (the required two files for any shiny app).   </p>

<ul>
<li>server.R</li>
<li>ui.R
[more details to be filled out]</li>
</ul>

<h3>
<a id="initr-file" class="anchor" href="#initr-file" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>init.r file</h3>

<ul>
<li>Keep in mind that '.r' in the file extension for init.r should be lowercase, as this is what is expected by the buildpack's compile script
[more details to be filled out]</li>
</ul>

<h3>
<a id="startscriptr-file" class="anchor" href="#startscriptr-file" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>startscript.r file</h3>

<ul>
<li>Contains R commands needed to start your shiny app -- in most cases this will include a runApp() function call.</li>
<li>Should be contained in the root of your app directory</li>
<li>Note that manifest.yml will point to this file
[more details to be filled out.  show example of startscript.r file]</li>
</ul>

<h3>
<a id="manifestyml-file" class="anchor" href="#manifestyml-file" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>manifest.yml file</h3>

<p>The manifest.yml file tells cf push what to do with your app by defining a set of 'attributes'. 'Attributes' include everything from how many instances to create, how much memory to allocate, and what command to run to start your app (i.e. for shiny, this could be the runApp() command).  In this section, we walk through the minimum set of attributes to include the 'applications' block of the manifest.yml that will get your shiny app on CF -- please refer to the <a href="http://docs.cloudfoundry.org/devguide/deploy-apps/manifest.html">CF documentation</a> for more details and information on additional attribute entries that you can include in the manifest.yml file. </p>

<ul>
<li>'name' attribute

<ul>
<li>The name of your app, preceded by a single dash and one space</li>
<li>Subsequent lines (i.e. attribute entries) in the manifest.yml file are indented with two spaces to align with 'name'</li>
<li>ex) <code>name: name_of_your_app</code>
</li>
</ul>
</li>
<li>'buildpack' attribute 

<ul>
<li>Here you can specify the URL of the buildpack</li>
<li>For shiny apps, it should point to a valid, compatible R buildpack for CF, i.e. git://github.com/wjjung317/heroku-buildpack-r.git</li>
<li>ex) <code>buildpack: git://github.com/wjjung317/heroku-buildpack-r.git</code>
</li>
</ul>
</li>
<li>'command' attribute

<ul>
<li>Inculdes commands required to start your Shiny app by pointing to the startscript.r file</li>
<li>ex) <code>command: R --no-save --gui-none &lt; /app/startscript.r</code>
</li>
</ul>
</li>
<li>'instances' attribute (optional)

<ul>
<li>Specify the number of app instances that you want to start upon push</li>
<li>If left blank, will be set to default value of 1</li>
<li>ex) <code>instances: 1</code>
</li>
</ul>
</li>
<li>'memory' attribute (optional)

<ul>
<li>Specify the memory limit for all instances of the app</li>
<li>If left blank, will be set to default value of 1G</li>
<li>ex) <code>memory: 1G</code>
</li>
</ul>
</li>
</ul>

<p>Using the examples provided above, below is a full template of a manifest.yml file that you can use as a suitable starting point for your shiny app:</p>

<pre><code>applications:
 - name: name_of_your_app
   buildpack: git://github.com/wjjung317/heroku-buildpack-r.git
   command: R --no-save --gui-none &lt; /app/startscript.r
   instances: 1
   memory: 1G
</code></pre>

<h2>
<a id="steps-to-push-your-shiny-app-to-cf-more-details-to-be-filled-out" class="anchor" href="#steps-to-push-your-shiny-app-to-cf-more-details-to-be-filled-out" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Steps to push your shiny app to CF [more details to be filled out]</h2>

<ol>
<li> Make sure your shiny app works on your laptop</li>
<li> Call out required R libraries and other initialization settings in your init.R file</li>
<li> Reference your shiny app's runApp() R script in the manifest.yml file</li>
</ol>

<h2>
<a id="common-mistakes-to-avoid" class="anchor" href="#common-mistakes-to-avoid" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Common mistakes to avoid</h2>

<ul>
<li>Don't assume that the latest version of buildpacks are good to go and when you push a new app or update an existing one -- you may need to mess around with the buildpack compile script.  After some trial-and-error, I needed to revert to an older version of the R buildpack as the latest version had a compatibility bug with one of the dependent libraries of Shiny.</li>
<li>When referring to file names in scripts that are used in your app, keep in mind case sensitivity of file names and file extensions.<br>
</li>
</ul>

<h1>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h1>

<ul>
<li>Woo Jung (<a href="https://github.com/wjjung317" class="user-mention">@wjjung317</a>)</li>
<li>Srivatsan 'Vatsan' Ramanujam (<a href="https://github.com/vatsan" class="user-mention">@vatsan</a>)</li>
<li>Noah Zimmerman (<a href="https://github.com/zimmeee" class="user-mention">@zimmeee</a>)</li>
<li>Alex Kagoshima (<a href="https://github.com/alexkago" class="user-mention">@alexkago</a>)</li>
<li>Ronert Obst (<a href="https://github.com/ronert" class="user-mention">@ronert</a>)</li>
<li>Regunathan Radhakrishnan (@regu_r)</li>
</ul>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/pivotalsoftware/gp-r">Pivotal | R</a> is maintained by <a href="https://github.com/pivotalsoftware">pivotalsoftware</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

            <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-39168204-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
